# Sistema Vida Grupo - Flujo: Carga Masiva de Riesgos y Beneficiarios üîÑ

## üìã **Introducci√≥n**

### Descripci√≥n del Flujo

La **Carga Masiva de Riesgos y Beneficiarios** es un proceso cr√≠tico de negocio que permite a los expedidores de p√≥lizas de Vida Grupo cargar de forma masiva (batch) informaci√≥n de asegurados y sus beneficiarios para p√≥lizas colectivas corporativas. Este flujo soporta operaciones de:

**Para Riesgos (Asegurados):**
- **Ingreso**: Creaci√≥n de nuevos riesgos (asegurados) en una p√≥liza master
- **Modificaci√≥n**: Actualizaci√≥n de informaci√≥n de riesgos existentes
- **Cancelaci√≥n**: Baja de riesgos de la p√≥liza
- **Renovaci√≥n**: Renovaci√≥n de riesgos en procesos de renovaci√≥n de p√≥liza

**Para Beneficiarios:**
- **Modificaci√≥n**: Creaci√≥n/Actualizaci√≥n de beneficiarios para riesgos existentes
- **Beneficiarios Completos**: Con tipo y n√∫mero de documento (se busca contacto existente)
- **Beneficiarios Dummy**: Sin identificaci√≥n (solo nombres y datos b√°sicos) - PersonDummy_Ext

El proceso est√° dise√±ado para manejar **vol√∫menes masivos** (desde cientos hasta miles de registros) de manera **completamente as√≠ncrona**, dividido en **dos fases independientes**:

- **Fase 1 (Azure)**: Validaci√≥n de estructura y formato del archivo mediante Azure Data Factory
- **Fase 2 (PolicyCenter)**: Validaci√≥n de reglas de negocio, transformaci√≥n e inserci√≥n de registros mediante Workqueues

Este dise√±o permite **procesamiento no bloqueante**, donde el usuario puede continuar trabajando mientras la carga se procesa en background, con visibilidad en tiempo real del estado mediante un dashboard interactivo.

### Scope del Documento

**Enfoque Principal**: Documentaci√≥n t√©cnica end-to-end del flujo de trabajo de Carga Masiva  
**Audiencia**: Desarrolladores, Arquitectos, Analistas de Negocio, DevOps  
**√öltima Actualizaci√≥n**: 28 de Enero, 2026

### Componentes Involucrados

| Componente         | Tecnolog√≠a   | Puerto/Contexto   | Responsabilidad               |
| ------------------ | ------------ | ----------------- | ----------------------------- |
| **PolicyCenter** | Gosu + Java + Guidewire 8.0.7 | http://localhost:8080/pc | Sistema de registro de p√≥lizas. Inicia carga, procesa Fase 2, gestiona UI del dashboard |
| **VidaGrupoIAC (Azure APIM)** | Azure API Management | https://api-vidagrupo-{env}-apigateway.azure-api.net | Gateway de APIs. Expone endpoints de Upload, Status, Download con autenticaci√≥n y rate limiting |
| **Azure Function Upload** | Node.js/C# Serverless | https://{env}Upload.azurewebsites.net | Recepci√≥n de bloques de archivo, almacenamiento en Storage Account |
| **Azure Function Status** | Node.js/C# Serverless | https://{env}Status.azurewebsites.net | Consulta y actualizaci√≥n de estados de carga en Cosmos DB |
| **Azure Function Download** | Node.js/C# Serverless | https://{env}Download.azurewebsites.net | Generaci√≥n y descarga de archivos de errores por fase |
| **Azure Data Factory** | PaaS Orchestration | N/A (servicio administrado) | Orquestaci√≥n de pipelines de validaci√≥n Fase 1 con expresiones regulares |
| **Cosmos DB** | NoSQL Database | N/A (servicio administrado) | Almacenamiento de registros de carga, errores de validaci√≥n, metadatos |
| **Azure Storage Account** | Blob Storage | N/A (servicio administrado) | Almacenamiento de archivos originales y archivos de errores generados |
| **RabbitMQ** | Message Broker | msglab.suramericana.com.co:5672 | Mensajer√≠a as√≠ncrona entre Azure y PolicyCenter para notificaciones de estado |

---

## üîÑ **Diagramas de Secuencia**

### 1. Flujo Principal: Carga Masiva End-to-End

```mermaid
sequenceDiagram
    participant U as Usuario Expedidor
    participant UI as PolicyCenter UI<br/>(PCF + JavaScript)
    participant APIM as Azure API Management
    participant FN_UP as Azure Function<br/>Massive Upload
    participant STORAGE as Azure Storage<br/>Blob Container
    participant DF as Azure Data Factory<br/>Pipeline Validation
    participant COSMOS as Cosmos DB<br/>MASSIVE_UPLOAD
    participant FN_ST as Azure Function<br/>Massive Status
    participant RMQ as RabbitMQ<br/>Messaging
    participant WQ1 as PolicyCenter<br/>Workqueue Enrichment
    participant WQ2 as PolicyCenter<br/>Workqueue BulkInsert
    participant EDGE as PolicyCenter<br/>Edge Services
    participant DB as PolicyCenter<br/>Oracle Database

    Note over U,DB: INICIO: Usuario carga archivo en PolicyCenter

    U->>UI: 1. Acceder a p√≥liza master ‚Üí Procesos Masivos
    UI->>UI: 2. Mostrar dashboard de historico (LifeCreateMassivelyInsuredsPopup.pcf)
    U->>UI: 3. Click en "Cargar" ‚Üí Seleccionar archivo CSV/Excel
    UI->>UI: 4. JavaScript: Calcular MD5 del archivo
    UI->>APIM: 5. GET uploadInProcessExists(policyNumber)
    APIM->>FN_ST: 5.1. Consultar cargas activas
    FN_ST->>COSMOS: 5.2. Query cargas en proceso
    COSMOS-->>FN_ST: 5.3. Resultado (vac√≠o o con carga)
    FN_ST-->>APIM: 5.4. 404 (sin carga) o 200 (carga activa)
    APIM-->>UI: 5.5. Respuesta validaci√≥n

    alt Carga en proceso detectada
        UI->>U: 6. Error: "P√≥liza procesando otra carga"
    else Sin carga en proceso
        UI->>UI: 7. Dividir archivo en bloques de 64KB
        
        loop Por cada bloque
            UI->>APIM: 8. PUT /file/{codOffer}/{operation}/{typeFile}/{policyId}/{md5}/blocks/{blockId}
            APIM->>FN_UP: 8.1. Enviar bloque
            FN_UP->>STORAGE: 8.2. Almacenar bloque en blob
            STORAGE-->>FN_UP: 8.3. Confirmaci√≥n
            FN_UP-->>APIM: 8.4. 200 OK
            APIM-->>UI: 8.5. Bloque enviado
            UI->>U: 8.6. Actualizar barra de progreso (X%)
        end

        UI->>APIM: 9. POST /file/{codOffer}/{operation}/{typeFile}/{policyId}/{md5}/blocks?blockCount=N
        APIM->>FN_UP: 9.1. Commit archivo
        FN_UP->>STORAGE: 9.2. Consolidar bloques en archivo √∫nico
        FN_UP->>COSMOS: 9.3. Crear registro de carga (status=UPLOADED)
        COSMOS-->>FN_UP: 9.4. Confirmaci√≥n
        FN_UP-->>APIM: 9.5. 201 Created
        APIM-->>UI: 9.6. Archivo cargado
        UI->>U: 9.7. "Archivo cargado exitosamente (100%)"
    end

    Note over STORAGE,DF: FASE 1: Validaci√≥n Azure Data Factory (As√≠ncrono)

    STORAGE->>DF: 10. Trigger: BlobCreated event
    DF->>DF: 11. Pipeline startFileValidation ejecuta
    DF->>STORAGE: 12. Leer archivo CSV/Excel
    STORAGE-->>DF: 13. Contenido del archivo
    
    loop Por cada registro del archivo
        DF->>DF: 14. Aplicar validaciones de formato (expresiones regulares)
        DF->>DF: 15. Validar estructura seg√∫n controlSchema
        
        alt Registro v√°lido
            DF->>COSMOS: 16. INSERT registro en container UPLOAD_DETAIL (status=VALIDATED)
        else Registro inv√°lido
            DF->>COSMOS: 17. INSERT error en container UPLOAD_DETAIL (status=ERROR_PHASE1)
        end
    end

    alt Fase 1 exitosa (sin errores cr√≠ticos)
        DF->>COSMOS: 18. UPDATE registro principal (status=PHASE1_COMPLETED)
        DF->>RMQ: 19. Publicar mensaje: NotifyPhaseStarted
        RMQ->>DB: 20. Insertar en NotifyPhaseMessage_Ext
        DF->>RMQ: 21. Publicar mensaje por registro: RecordReadyForProcessing
        
        loop Por cada registro v√°lido
            RMQ->>WQ1: 22. Consumir mensaje de registro
            WQ1->>DB: 23. INSERT en MassiveUploadMessage_Ext (status=PENDING_SEND)
        end
    else Fase 1 con errores
        DF->>STORAGE: 24. Generar archivo ErrorFase1_{md5}.csv
        DF->>COSMOS: 25. UPDATE registro (status=ERROR_PHASE1, linkErrors=urlArchivo)
        DF->>RMQ: 26. Publicar NotifyPhaseError
    end

    Note over WQ1,DB: FASE 2: Procesamiento PolicyCenter (As√≠ncrono)

    WQ1->>WQ1: 27. Workqueue Enrichment inicia (scheduled)
    WQ1->>DB: 28. SELECT * FROM MassiveUploadMessage_Ext WHERE status=PENDING_SEND
    DB-->>WQ1: 29. Registros pendientes
    
    loop Por cada registro pendiente
        WQ1->>APIM: 30. GET /massivestatus/{policyNumber}/{md5}
        APIM->>FN_ST: 30.1. Consultar detalle
        FN_ST->>COSMOS: 30.2. Query registro completo
        COSMOS-->>FN_ST: 30.3. Datos del registro
        FN_ST-->>APIM: 30.4. 200 OK + JSON data
        APIM-->>WQ1: 30.5. Detalles del registro
        
        WQ1->>WQ1: 31. Validar campos de negocio (OperationValidationFactory)
        
        alt Validaci√≥n exitosa
            WQ1->>WQ1: 32. Transformar datos (TransformerFactoryProducer)
            WQ1->>DB: 33. UPDATE MassiveUploadMessage_Ext (status=ACKED, response=transformedData)
            WQ1->>APIM: 34. PUT /massivestatus/{policyNumber}/{md5} (status=PROCESSING_PHASE2)
            APIM->>FN_ST: 34.1. Actualizar estado
            FN_ST->>COSMOS: 34.2. UPDATE estado
        else Validaci√≥n fallida
            WQ1->>DB: 35. UPDATE MassiveUploadMessage_Ext (status=ERROR, errorMessage=...)
            WQ1->>APIM: 36. PUT /massivestatus/{policyNumber}/{md5} (status=ERROR_PHASE2)
            WQ1->>STORAGE: 37. Agregar error a archivo ErrorFase2_{md5}.csv
        end
    end

    Note over WQ2,EDGE: FASE 2: Bulk Insert seg√∫n Volumen

    WQ2->>WQ2: 38. Workqueue BulkInsert inicia (por volumen: Low/Medium/High)
    WQ2->>DB: 39. SELECT * FROM MassiveUploadMessage_Ext WHERE status=ACKED
    DB-->>WQ2: 40. Registros listos para insertar
    
    loop Por cada registro ACKED
        WQ2->>EDGE: 41. Invocar Edge Service seg√∫n operaci√≥n
        
        alt Operaci√≥n: Ingreso (Submission)
            EDGE->>DB: 42. Crear nueva p√≥liza/riesgo (INSERT)
        else Operaci√≥n: Modificaci√≥n (PolicyChange)
            EDGE->>DB: 43. Actualizar riesgo existente (UPDATE)
        else Operaci√≥n: Cancelaci√≥n (Cancellation)
            EDGE->>DB: 44. Cancelar riesgo (UPDATE + estado)
        else Operaci√≥n: Renovaci√≥n (Renewal)
            EDGE->>DB: 45. Renovar riesgo (INSERT nueva versi√≥n)
        end
        
        DB-->>EDGE: 46. Resultado operaci√≥n
        
        alt Inserci√≥n exitosa
            EDGE-->>WQ2: 47. Success
            WQ2->>DB: 48. UPDATE MassiveUploadMessage_Ext (status=COMPLETED)
            WQ2->>APIM: 49. Notificar √©xito a Azure
            APIM->>FN_ST: 49.1. Actualizar contador
            FN_ST->>COSMOS: 49.2. INCREMENT successful_count
        else Inserci√≥n fallida
            EDGE-->>WQ2: 50. Error (timeout, constraint violation, etc.)
            WQ2->>DB: 51. UPDATE MassiveUploadMessage_Ext (status=ERROR)
            WQ2->>APIM: 52. Notificar error a Azure
            APIM->>FN_ST: 52.1. Actualizar contador
            FN_ST->>COSMOS: 52.2. INCREMENT error_count
        end
    end

    Note over U,DB: FINALIZACI√ìN: Usuario consulta resultados

    U->>UI: 53. Click en "Refrescar Dashboard"
    UI->>APIM: 54. GET /massivestatus/{policyNumber}?page=1&size=10
    APIM->>FN_ST: 54.1. Listar cargas con paginaci√≥n
    FN_ST->>COSMOS: 54.2. Query cargas ordenadas por fecha
    COSMOS-->>FN_ST: 54.3. Lista de cargas
    FN_ST-->>APIM: 54.4. 200 OK + JSON array
    APIM-->>UI: 54.5. Datos del dashboard
    UI->>U: 54.6. Mostrar tabla con MD5, fecha, estado, registros exitosos/fallidos

    alt Hay errores en alguna fase
        U->>UI: 55. Click en "Descargar Errores Fase X"
        UI->>APIM: 56. GET /massive-download/file/output-errors/...
        APIM->>FN_DOWN: 56.1. Solicitar archivo de errores
```

### 1.1. Flujo Espec√≠fico: Carga Masiva de Beneficiarios

```mermaid
sequenceDiagram
    participant U as Usuario Expedidor
    participant WQ1 as Workqueue Enrichment
    participant VAL as DefaultBeneficiaryValidation
    participant TRANS as DefaultBeneficiaryTransformer
    participant FACTORY as ContactCreationStrategyFactory
    participant DUMMY as DummyContactCreationStrategy
    participant COMPLETE as CompleteContactCreationStrategy
    participant COMMON as CommonTransformer
    participant DB as Oracle Database

    Note over U,DB: FASE 2: Procesamiento de Beneficiarios en PolicyCenter

    WQ1->>VAL: 1. Validar registro de beneficiario
    VAL->>VAL: 2. validatePrimaryBeneficiary()
    Note right of VAL: Campos obligatorios:<br/>- PRIMER_NOMBRE<br/>- PRIMER_APELLIDO<br/>- PARENTESCO<br/>- PORCENTAJE<br/>- CELULAR<br/>(TIPO_DOC y NUM_DOC son OPCIONALES)
    
    VAL->>VAL: 3. validateNumberAndTypeDocumentBeneficiary()
    
    alt Ambos campos vac√≠os (Beneficiario Dummy)
        VAL->>VAL: 4a. Validaci√≥n OK - Continuar como Dummy
    else Ambos campos llenos (Beneficiario Completo)
        VAL->>VAL: 4b. matchTypeAndNumberDocument(regex)
        VAL->>VAL: 4c. Validaci√≥n OK - Continuar como Completo
    else Un campo vac√≠o y otro lleno (Error)
        VAL->>VAL: 4d. addFieldError("beneficiaryDocumentIncomplete")
        VAL-->>WQ1: 4e. ValidationException
    end
    
    WQ1->>TRANS: 5. transform(requestDTO, policyData)
    TRANS->>TRANS: 6. createAdditionalInterest()
    
    loop Por cada beneficiario (1-5)
        TRANS->>FACTORY: 7. getStrategy(documentType, documentNumber)
        
        alt Tipo y N√∫mero vac√≠os
            FACTORY->>DUMMY: 8a. DummyContactCreationStrategy
            DUMMY->>DUMMY: 9a. determineContactType() ‚Üí TC_LFPERSONDUMMY
            DUMMY->>DUMMY: 10a. new PersonDummy_Ext()
            DUMMY->>DUMMY: 11a. populateContactData(firstName, lastName, etc.)
            DUMMY->>DUMMY: 12a. createAddressDummy()
            DUMMY-->>TRANS: 13a. PersonDummy_Ext contact
        else Tipo y N√∫mero llenos
            FACTORY->>COMPLETE: 8b. CompleteContactCreationStrategy
            COMPLETE->>DB: 9b. searchContact(documentType, documentNumber)
            DB-->>COMPLETE: 10b. Contact existente o null
            COMPLETE-->>TRANS: 11b. Contact o null
        end
        
        TRANS->>COMMON: 14. contactToAdditionalInterest(contact, requestDTO, number)
        
        alt Contact es PersonDummy_Ext
            COMMON->>COMMON: 15a. setPersonDummyAdditionalInterestData()
            COMMON->>COMMON: 16a. Mapear: FirstName, MiddleName, LastName, Particle
            COMMON->>COMMON: 17a. toPrimaryAddress(contact.PrimaryAddress)
        else Contact es Person/Company est√°ndar
            COMMON->>COMMON: 15b. setPersonAdditionalInterestData() o setCompanyAdditionalInterestData()
            COMMON->>COMMON: 16b. Mapear datos desde CSV usando BeneficiaryTemplate
        end
        
        COMMON-->>TRANS: 18. AdditionalInterestDTO
        TRANS->>TRANS: 19. Asignar: BeneficiaryType, Relationship, ParticipationPercentage
    end
    
    TRANS-->>WQ1: 20. PolicyDataExtDTO con AdditionalInterest[]
    WQ1->>WQ1: 21. notifySuccess() ‚Üí Guardar en MassiveUploadMessage_Ext.Response
```

### Tabla de Decisi√≥n: Creaci√≥n de Beneficiarios

| Tipo Documento | N√∫mero Documento | Estrategia | Resultado |
|----------------|------------------|------------|-----------|
| Vac√≠o | Vac√≠o | `DummyContactCreationStrategy` | Crea `PersonDummy_Ext` con datos b√°sicos (nombres, direcci√≥n dummy) |
| Lleno | Lleno | `CompleteContactCreationStrategy` | Busca contacto existente en BD, si no existe usa datos del CSV |
| Vac√≠o | Lleno | N/A | ‚ùå Error: "El tipo de documento y n√∫mero de documento del beneficiario deben estar ambos diligenciados o ambos vac√≠os" |
| Lleno | Vac√≠o | N/A | ‚ùå Error: "El tipo de documento y n√∫mero de documento del beneficiario deben estar ambos diligenciados o ambos vac√≠os" |

### Arquitectura de Estrategias (Strategy Pattern)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PATR√ìN STRATEGY PARA BENEFICIARIOS                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ  ContactCreationStrategyFactory                                         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ getStrategy(documentType, documentNumber)                          ‚îÇ
‚îÇ      ‚îÇ                                                                  ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ if (ambos vac√≠os) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ DummyContactCreationStrategy   ‚îÇ
‚îÇ      ‚îÇ                                   ‚îú‚îÄ‚îÄ createContact()            ‚îÇ
‚îÇ      ‚îÇ                                   ‚îÇ   ‚îî‚îÄ‚îÄ new PersonDummy_Ext()  ‚îÇ
‚îÇ      ‚îÇ                                   ‚îî‚îÄ‚îÄ populateContactData()      ‚îÇ
‚îÇ      ‚îÇ                                                                  ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ if (ambos llenos) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ CompleteContactCreationStrategy‚îÇ
‚îÇ      ‚îÇ                                   ‚îú‚îÄ‚îÄ createContact()            ‚îÇ
‚îÇ      ‚îÇ                                   ‚îÇ   ‚îî‚îÄ‚îÄ searchContact()        ‚îÇ
‚îÇ      ‚îÇ                                   ‚îî‚îÄ‚îÄ return Contact o null      ‚îÇ
‚îÇ      ‚îÇ                                                                  ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ if (mixto) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ DisplayableException           ‚îÇ
‚îÇ                                          ‚îî‚îÄ‚îÄ "Documento incompleto"     ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
        FN_DOWN->>STORAGE: 56.2. Leer ErrorFaseX_{md5}.csv
        STORAGE-->>FN_DOWN: 56.3. Contenido del archivo
        FN_DOWN-->>APIM: 56.4. 200 OK + CSV data
        APIM-->>UI: 56.5. Archivo CSV
        UI->>U: 56.6. Descargar ErrorFase1_{md5}.csv
    end
```

### 2. Flujo de Manejo de Errores

```mermaid
sequenceDiagram
    participant UI as PolicyCenter UI
    participant APIM as Azure API Management
    participant FN_ST as Function Status
    participant COSMOS as Cosmos DB
    participant DF as Data Factory
    participant WQ as Workqueue
    participant STORAGE as Storage Account

    Note over UI,STORAGE: ESCENARIO 1: Error en Validaci√≥n Fase 1 (DataFactory)

    DF->>DF: Validar registro con regex
    DF->>DF: Detectar formato inv√°lido (ej: fecha futura, campo vac√≠o)
    DF->>COSMOS: INSERT error detail (phase=1, errorMessage="Campo X inv√°lido")
    DF->>DF: Incrementar contador de errores
    
    alt Todos los registros con error
        DF->>COSMOS: UPDATE carga (status=ERROR_PHASE1)
        DF->>STORAGE: Generar ErrorFase1_{md5}.csv
        DF->>COSMOS: UPDATE linkErrors con URL de archivo
    else Errores parciales
        DF->>COSMOS: UPDATE carga (status=PHASE1_PARTIAL_ERROR)
        DF->>STORAGE: Generar ErrorFase1_{md5}.csv (solo errores)
        DF->>DF: Continuar con registros v√°lidos a Fase 2
    end

    Note over UI,STORAGE: ESCENARIO 2: Timeout en Azure Upload

    UI->>APIM: PUT /file/.../blocks/{blockId}
    APIM->>FN_ST: Enviar bloque
    FN_ST->>FN_ST: Procesamiento lento (red, tama√±o)
    
    alt Timeout antes de respuesta
        FN_ST--xUI: Timeout (sin respuesta)
        UI->>UI: Detectar timeout
        UI->>APIM: GET /file/.../blocks?asCount=true
        APIM->>FN_ST: Consultar √∫ltimo bloque recibido
        FN_ST->>COSMOS: Query lastBlockId
        COSMOS-->>FN_ST: blockId=X
        FN_ST-->>APIM: 200 OK {lastBlock: X}
        APIM-->>UI: √öltimo bloque confirmado
        UI->>UI: Reanudar desde bloque X+1
        UI->>APIM: PUT /file/.../blocks/{X+1}
    else Respuesta exitosa despu√©s de demora
        FN_ST-->>APIM: 200 OK (con delay)
        APIM-->>UI: Bloque recibido
        UI->>UI: Continuar con siguiente bloque
    end

    Note over UI,STORAGE: ESCENARIO 3: Error en Validaci√≥n Fase 2 (PolicyCenter)

    WQ->>WQ: OperationValidationFactory.validate()
    WQ->>WQ: Detectar error de negocio (ej: riesgo no existe para beneficiario)
    WQ->>WQ: Lanzar ValidationException
    
    alt Error recuperable (reintentable)
        WQ->>WQ: Verificar attempts < maxAttempts
        WQ->>WQ: Incrementar attempts
        WQ->>WQ: Re-encolar registro (retry despu√©s de delay)
    else Error no recuperable
        WQ->>COSMOS: UPDATE registro (status=ERROR_PHASE2, errorMessage="...")
        WQ->>STORAGE: Append error a ErrorFase2_{md5}.csv
        WQ->>APIM: PUT /massivestatus/{policyNumber}/{md5} (actualizar contador errores)
        APIM->>FN_ST: Actualizar estad√≠sticas
        FN_ST->>COSMOS: INCREMENT error_count_phase2
    end

    Note over UI,STORAGE: ESCENARIO 4: Fallo en Servicios Edge

    WQ->>WQ: EdgeProcessing.process()
    WQ->>WQ: Invocar Edge Service (Submission/Change/Renewal)
    WQ->>WQ: Edge Service timeout o exception
    
    alt Timeout Edge Service
        WQ->>WQ: Catch TimeoutException
        WQ->>WQ: Verificar attempts < maxAttempts (ej: 3 reintentos)
        
        alt Reintentos disponibles
            WQ->>WQ: Incrementar attempts, re-encolar
        else Reintentos agotados
            WQ->>COSMOS: Marcar ERROR_EDGE_TIMEOUT
            WQ->>STORAGE: Agregar a archivo ErrorFase2
        end
    else Exception cr√≠tica Edge
        WQ->>WQ: Catch Exception
        WQ->>COSMOS: Marcar ERROR_EDGE_EXCEPTION
        WQ->>STORAGE: Agregar detalle de exception a ErrorFase2
    end

    Note over UI,STORAGE: ESCENARIO 5: Estado Inconsistente (Carga Finalizada en Policy, En Progreso en Azure)

    UI->>UI: Usuario reporta carga "atascada"
    UI->>APIM: GET /massivestatus/{policyNumber}/{md5}
    APIM->>FN_ST: Consultar estado
    FN_ST->>COSMOS: Query estado actual
    COSMOS-->>FN_ST: status=PROCESSING (inconsistente)
    FN_ST-->>APIM: 200 OK {status: "En Progreso"}
    APIM-->>UI: Estado: "En Progreso"
    
    UI->>UI: Operador detecta inconsistencia
    UI->>APIM: PUT /massivestatus/{policyNumber}/{md5} (force_complete=true)
    APIM->>FN_ST: Forzar actualizaci√≥n de estado
    FN_ST->>COSMOS: UPDATE status=COMPLETED (manual override)
    COSMOS-->>FN_ST: Confirmaci√≥n
    FN_ST-->>APIM: 200 OK
    APIM-->>UI: Estado actualizado
    UI->>UI: Refrescar dashboard ‚Üí mostrar "Finalizada"
```

### 3. Flujo de Operaciones As√≠ncronas

```mermaid
sequenceDiagram
    participant UI as Usuario
    participant JS as JavaScript Client<br/>(massiveLoad.js)
    participant APIM as API Gateway
    participant FN_UP as Azure Function Upload
    participant STORAGE as Storage Account
    participant DF as Data Factory<br/>(Trigger Autom√°tico)
    participant COSMOS as Cosmos DB
    participant RMQ as RabbitMQ
    participant WQ1 as Workqueue Enrichment<br/>(Scheduled Job)
    participant WQ2 as Workqueue BulkInsert<br/>(Scheduled Job)

    Note over UI,WQ2: PROCESAMIENTO AS√çNCRONO - NO BLOQUEANTE

    UI->>JS: Seleccionar archivo y Click "Cargar"
    JS->>JS: Calcular MD5 + Dividir en bloques (async)
    
    par Env√≠o de bloques (paralelo no bloqueante)
        loop Bloque 1 a N
            JS->>APIM: XMLHttpRequest async PUT /blocks/{blockId}
            APIM->>FN_UP: Procesar bloque
            FN_UP->>STORAGE: Write blob chunk
            STORAGE-->>FN_UP: OK
            FN_UP-->>APIM: 200 OK
            APIM-->>JS: Callback success
            JS->>JS: Actualizar progress bar (no bloquea UI)
        end
    and Usuario contin√∫a trabajando
        UI->>UI: Usuario puede navegar, consultar dashboard, etc.
    end

    JS->>APIM: POST /blocks (commit final)
    APIM->>FN_UP: Consolidar archivo
    FN_UP->>STORAGE: Commit blob (operaci√≥n at√≥mica)
    STORAGE-->>FN_UP: Blob URI
    FN_UP-->>APIM: 201 Created
    APIM-->>JS: Archivo cargado
    JS->>UI: Notificaci√≥n: "Archivo cargado, procesamiento iniciado"
    JS->>UI: Cerrar modal, volver a dashboard

    Note over STORAGE,COSMOS: PROCESAMIENTO EN BACKGROUND - FASE 1

    STORAGE->>DF: BlobCreated Event (trigger autom√°tico)
    activate DF
    DF->>DF: Pipeline startFileValidation ejecuta
    DF->>STORAGE: Leer archivo en chunks
    
    loop Procesar lotes de 100 registros
        DF->>DF: Validar lote con dataflow
        DF->>COSMOS: Batch INSERT registros validados
    end
    
    DF->>COSMOS: UPDATE metadata (status=PHASE1_COMPLETED)
    DF->>RMQ: Publicar eventos NotifyPhase (async)
    deactivate DF

    Note over RMQ,WQ1: COLA DE MENSAJER√çA AS√çNCRONA

    RMQ->>WQ1: Consumir mensaje (pull o push)
    WQ1->>WQ1: Crear trabajo en cola (no procesado inmediatamente)

    Note over WQ1,WQ2: PROCESAMIENTO PROGRAMADO (SCHEDULED JOBS)

    loop Cada 5 minutos (configurable)
        activate WQ1
        WQ1->>WQ1: Scheduled trigger ejecuta
        WQ1->>WQ1: Procesar batch de 50 registros PENDING_SEND
        
        par Procesamiento de registros (paralelo interno)
            WQ1->>WQ1: Validar registro 1
            WQ1->>WQ1: Transformar registro 1
        and
            WQ1->>WQ1: Validar registro 2
            WQ1->>WQ1: Transformar registro 2
        and
            WQ1->>WQ1: Validar registro 3...N
        end
        
        WQ1->>WQ1: Marcar registros como ACKED
        deactivate WQ1
    end

    loop Cada 5 minutos (por volumen: Low/Medium/High)
        activate WQ2
        WQ2->>WQ2: Scheduled trigger ejecuta (seg√∫n volumen)
        WQ2->>WQ2: Procesar batch de registros ACKED
        
        loop Por cada registro en batch
            WQ2->>WQ2: Invocar Edge Service
            WQ2->>WQ2: Persistir resultado
        end
        
        WQ2->>WQ2: Marcar registros como COMPLETED/ERROR
        WQ2->>COSMOS: Actualizar estad√≠sticas (async via RabbitMQ)
        deactivate WQ2
    end

    Note over UI,COSMOS: CONSULTA DE ESTADO (NO BLOQUEANTE)

    loop Usuario refresca dashboard peri√≥dicamente
        UI->>APIM: GET /massivestatus/{policyNumber}
        APIM->>COSMOS: Query estados
        COSMOS-->>APIM: Snapshot actual
        APIM-->>UI: Estado en tiempo real
        UI->>UI: Actualizar tabla dashboard
    end
```

---

## üèóÔ∏è **Componentes de PolicyCenter para Beneficiarios**

### Estructura de Archivos

```
PolicyCenter/modules/configuration/gsrc/sura/pc/massiveupload/
‚îú‚îÄ‚îÄ batch/
‚îÇ   ‚îî‚îÄ‚îÄ enrichment/
‚îÇ       ‚îî‚îÄ‚îÄ EnrichmentProcessing.gs          # Orquestador principal de Fase 2
‚îú‚îÄ‚îÄ strategy/                                  # Patr√≥n Strategy (NUEVO)
‚îÇ   ‚îú‚îÄ‚îÄ ContactCreationStrategy.gs            # Interface del patr√≥n
‚îÇ   ‚îú‚îÄ‚îÄ ContactCreationStrategyFactory.gs     # Factory que decide estrategia
‚îÇ   ‚îú‚îÄ‚îÄ DummyContactCreationStrategy.gs       # Crea PersonDummy_Ext
‚îÇ   ‚îî‚îÄ‚îÄ CompleteContactCreationStrategy.gs    # Busca contacto existente
‚îú‚îÄ‚îÄ transformer/
‚îÇ   ‚îú‚îÄ‚îÄ DefaultBeneficiaryTransformer.gs      # Transformador de beneficiarios
‚îÇ   ‚îú‚îÄ‚îÄ common/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CommonTransformer.gs              # Mapeo de contactos a DTOs
‚îÇ   ‚îî‚îÄ‚îÄ factories/
‚îÇ       ‚îú‚îÄ‚îÄ TransformerFactoryProducer.gs     # Produce factories por tipo archivo
‚îÇ       ‚îî‚îÄ‚îÄ BeneficiaryTransformerFactory.gs  # Crea transformadores de beneficiarios
‚îî‚îÄ‚îÄ validations/
    ‚îú‚îÄ‚îÄ OperationValidationBase.gs            # Validaciones base
    ‚îî‚îÄ‚îÄ factories/
        ‚îú‚îÄ‚îÄ OperationValidationFactory.gs     # Produce validadores por operaci√≥n
        ‚îî‚îÄ‚îÄ DefaultBeneficiaryValidation.gs   # Validaciones espec√≠ficas beneficiarios
```

### Responsabilidades de Componentes

| Componente | Responsabilidad | Cambios para Dummy |
|------------|-----------------|-------------------|
| `EnrichmentProcessing` | Orquestador: valida ‚Üí transforma ‚Üí notifica | Sin cambios (gen√©rico) |
| `OperationValidationFactory` | Crea validador seg√∫n tipo archivo y operaci√≥n | Limpieza c√≥digo deprecado |
| `DefaultBeneficiaryValidation` | Valida campos obligatorios y formato de documento | TIPO_DOC y NUM_DOC ahora opcionales, validaci√≥n condicional |
| `TransformerFactoryProducer` | Crea factory de transformadores seg√∫n tipo archivo | Limpieza c√≥digo deprecado |
| `DefaultBeneficiaryTransformer` | Transforma CSV a PolicyDataExtDTO con beneficiarios | Usa `ContactCreationStrategyFactory` |
| `ContactCreationStrategyFactory` | Decide qu√© estrategia usar seg√∫n campos de documento | **NUEVO** - Implementa tabla de decisi√≥n |
| `DummyContactCreationStrategy` | Crea `PersonDummy_Ext` con datos b√°sicos | **NUEVO** - Reutiliza l√≥gica existente |
| `CompleteContactCreationStrategy` | Busca contacto existente por documento | **NUEVO** - Encapsula b√∫squeda |
| `CommonTransformer` | Mapea Contact a AdditionalInterestDTO | Soporte para `PersonDummy_Ext` |

### Entidades de Guidewire Utilizadas

| Entidad | Tipo | Prop√≥sito en Carga Masiva |
|---------|------|---------------------------|
| `MassiveUploadMessage_Ext` | Entity | Almacena registro CSV, estado, request/response de cada registro |
| `PersonDummy_Ext` | Entity (extiende Person) | Beneficiario persona sin identificaci√≥n formal |
| `CompanyDummy_Ext` | Entity (extiende Company) | Beneficiario empresa sin identificaci√≥n formal (no usado en beneficiarios) |
| `PolicyPeriod` | Entity (core) | Per√≠odo de p√≥liza donde se agregan beneficiarios |
| `Contact` | Entity (core) | Clase base para Person, Company, PersonDummy_Ext |

---

## üìä **Estados y Transiciones**

### Diagrama de Estados del Flujo

```mermaid
stateDiagram-v2
    [*] --> UPLOADING: Usuario inicia carga de archivo
    UPLOADING --> UPLOADED: Todos los bloques enviados exitosamente
    UPLOADING --> ERROR_UPLOAD: Timeout o error en env√≠o de bloques
    
    UPLOADED --> VALIDATING_PHASE1: Data Factory inicia validaci√≥n
    VALIDATING_PHASE1 --> PHASE1_COMPLETED: Validaci√≥n exitosa (sin errores cr√≠ticos)
    VALIDATING_PHASE1 --> PHASE1_PARTIAL_ERROR: Validaci√≥n con errores parciales
    VALIDATING_PHASE1 --> ERROR_PHASE1: Validaci√≥n fallida (todos los registros con error)
    
    PHASE1_COMPLETED --> PROCESSING_PHASE2: Workqueue Enrichment inicia
    PHASE1_PARTIAL_ERROR --> PROCESSING_PHASE2: Registros v√°lidos contin√∫an a Fase 2
    
    PROCESSING_PHASE2 --> ENRICHED: Validaci√≥n y transformaci√≥n exitosa
    PROCESSING_PHASE2 --> ERROR_PHASE2: Error en validaci√≥n de negocio
    
    ENRICHED --> INSERTING: Workqueue BulkInsert inicia
    INSERTING --> COMPLETED: Todos los registros insertados exitosamente
    INSERTING --> PARTIAL_COMPLETED: Algunos registros insertados, otros con error
    INSERTING --> ERROR_INSERT: Error cr√≠tico en inserci√≥n (Edge timeout, DB constraint)
    
    COMPLETED --> [*]: Proceso finalizado exitosamente
    PARTIAL_COMPLETED --> [*]: Proceso finalizado con errores parciales
    ERROR_UPLOAD --> [*]: Usuario debe reintentar carga
    ERROR_PHASE1 --> [*]: Usuario descarga errores, corrige archivo y reintenta
    ERROR_PHASE2 --> [*]: Usuario descarga errores, analiza y reintenta registros fallidos
    ERROR_INSERT --> [*]: Requiere intervenci√≥n t√©cnica
    
    ERROR_PHASE1 --> REINTENTO: Usuario sube archivo corregido
    ERROR_PHASE2 --> REINTENTO: Usuario sube archivo corregido
    REINTENTO --> UPLOADING: Nueva carga iniciada
```

---

## üìã **Configuraci√≥n y Par√°metros**

### Configuraci√≥n del Flujo

| Par√°metro         | Valor         | Descripci√≥n             | Impacto si se Cambia |
| ----------------- | ------------- | ----------------------- | -------------------- |
| `BLOCK_SIZE` | `64 KB` | Tama√±o de cada bloque para env√≠o a Azure | Bloques m√°s grandes: menos requests pero mayor riesgo de timeout. Bloques m√°s peque√±os: m√°s requests pero m√°s resiliente |
| `WORKQUEUE_BATCH_SIZE_ENRICHMENT` | `50 registros` | Cantidad de registros procesados por ejecuci√≥n del Workqueue Enrichment | Valores altos: procesamiento m√°s r√°pido pero mayor carga en DB. Valores bajos: procesamiento m√°s lento pero menor impacto |
| `WORKQUEUE_SCHEDULE_ENRICHMENT` | `Cada 5 minutos` | Frecuencia de ejecuci√≥n del Workqueue Enrichment | Menor frecuencia: procesamiento m√°s lento. Mayor frecuencia: m√°s carga en sistema |
| `WORKQUEUE_SCHEDULE_BULKINSERT_LOW` | `Cada 5 min (Lun-Vie 5-18h)` | Schedule para cargas peque√±as (0-100 registros) | Cambiar horario afecta ventana de procesamiento |
| `WORKQUEUE_SCHEDULE_BULKINSERT_MEDIUM` | `Cada 5 min (Lun-Vie 5-18h)` | Schedule para cargas medianas (101-2000 registros) | Cambiar horario afecta ventana de procesamiento |
| `WORKQUEUE_SCHEDULE_BULKINSERT_HIGH` | `Cada 5 min (Lun-Vie 5-18h)` | Schedule para cargas grandes (2001+ registros) | Cambiar horario afecta ventana de procesamiento |
| `MAX_RETRY_ATTEMPTS_ENRICHMENT` | `3 reintentos` | N√∫mero m√°ximo de reintentos para validaci√≥n/transformaci√≥n | M√°s reintentos: mayor resiliencia pero registros problem√°ticos tardan m√°s en marcar como error |
| `MAX_RETRY_ATTEMPTS_BULKINSERT` | `3 reintentos` | N√∫mero m√°ximo de reintentos para inserci√≥n Edge | M√°s reintentos: mayor resiliencia para errores transitorios |
| `COSMOS_TTL_RECORDS` | `90 d√≠as` | Time To Live de registros en Cosmos DB | Mayor TTL: m√°s almacenamiento. Menor TTL: p√©rdida de hist√≥rico |
| `PAGINATION_SIZE_DASHBOARD` | `10 cargas` | Cantidad de cargas mostradas por p√°gina en dashboard | Valores altos: m√°s carga en renderizado UI. Valores bajos: m√°s paginaci√≥n |

### Message Queues Utilizadas

| Cola             | Exchange     | Routing Key     | TTL     | Prop√≥sito               |
| ---------------- | ------------ | --------------- | ------- | ----------------------- |
| `massive.upload.notify.phase` | `sura.seguros.massive` | `notify.phase.started` | `30 d√≠as` | Notificar inicio de fase desde Azure a PolicyCenter |
| `massive.upload.record.ready` | `sura.seguros.massive` | `record.ready.processing` | `30 d√≠as` | Notificar registro listo para Workqueue Enrichment |
| `massive.upload.status.update` | `sura.seguros.massive` | `status.update` | `7 d√≠as` | Actualizar estado de registro en Azure desde PolicyCenter (cola 44) |

---

## üîß **M√©tricas y Monitoreo**

### Puntos Cr√≠ticos de Medici√≥n

| M√©trica                    | Componente             | Umbral Esperado | Acci√≥n si se Excede    |
| -------------------------- | ---------------------- | --------------- | ---------------------- |
| **Tiempo de carga de archivo** | JavaScript + Azure Function Upload | < 2 minutos por cada 10MB | Revisar latencia de red, tama√±o de bloques, concurrencia de requests |
| **Duraci√≥n Fase 1 (DataFactory)** | Azure Data Factory | < 10 minutos por cada 1000 registros | Optimizar pipelines, aumentar paralelismo, revisar expresiones regulares |
| **Duraci√≥n Fase 2 (Enrichment)** | Workqueue Enrichment | < 5 minutos por cada 100 registros | Optimizar validaciones, revisar queries a Cosmos/Oracle, aumentar batch size |
| **Duraci√≥n Fase 2 (BulkInsert)** | Workqueue BulkInsert | < 10 minutos por cada 100 registros | Optimizar servicios Edge, revisar performance de DB, aumentar paralelismo |
| **Tasa de error Fase 1** | Data Factory | < 5% de registros con error | Revisar calidad de archivos fuente, mejorar documentaci√≥n para usuarios |
| **Tasa de error Fase 2** | Workqueue Enrichment | < 2% de registros con error | Revisar validaciones de negocio, capacitar a usuarios en reglas |
| **Latencia API Massive Status** | Azure Function Status | < 500ms (p95) | Optimizar queries Cosmos DB, implementar cache, revisar √≠ndices |
| **Latencia API Massive Upload** | Azure Function Upload | < 1s por bloque (p95) | Revisar performance Storage Account, optimizar escritura de blobs |

### Logs Cr√≠ticos a Monitorear

| Componente        | Archivo Log     | Patr√≥n a Buscar       | Severidad |
| ----------------- | --------------- | --------------------- | --------- |
| **PolicyCenter Workqueue** | `PolicyCenter.log` | `ERROR.*MassiveUploadMessageProcess` | ERROR |
| **PolicyCenter Workqueue** | `PolicyCenter.log` | `ERROR.*EnrichmentProcessing` | ERROR |
| **PolicyCenter Workqueue** | `PolicyCenter.log` | `ERROR.*EdgeProcessing` | ERROR |
| **Azure Function Upload** | Application Insights | `exceptions` where `cloud_RoleName` contains "Upload" | ERROR |
| **Azure Function Status** | Application Insights | `exceptions` where `cloud_RoleName` contains "Status" | ERROR |
| **Azure Data Factory** | Monitor Logs | `Pipeline Run Failed` | ERROR |
| **RabbitMQ** | RabbitMQ Management | `Messages Unacked > 100` | WARN |
| **Cosmos DB** | Metrics | `429 Throttling Errors` | WARN |

---

## üß™ **Escenarios de Prueba**

### Casos de Prueba Cr√≠ticos

#### TC001: Flujo Exitoso Completo - Carga de Riesgos (Ingreso)

```gherkin
Scenario: Carga masiva exitosa de 100 riesgos nuevos en p√≥liza Vida Grupo Integral
  Given el usuario es un expedidor autenticado en PolicyCenter
  And existe una p√≥liza master de Vida Grupo Integral activa con n√∫mero "VG-2025-001"
  And el usuario tiene un archivo CSV "riesgos_ingreso_100.csv" con 100 registros v√°lidos
  And el archivo contiene: identificaci√≥n, nombre, fecha nacimiento, suma asegurada, coberturas
  And no hay otra carga en proceso para la p√≥liza "VG-2025-001"
  
  When el usuario navega a la p√≥liza master "VG-2025-001"
  And hace click en "Procesos Masivos" ‚Üí "Cargar Masivamente"
  And selecciona "Tipo de Archivo: Riesgo"
  And selecciona "Operaci√≥n: Ingreso"
  And sube el archivo "riesgos_ingreso_100.csv"
  
  Then el sistema debe calcular el MD5 del archivo
  And debe enviar 100+ bloques de 64KB a Azure Massive Upload API
  And debe mostrar barra de progreso llegando al 100%
  And debe mostrar mensaje "Archivo cargado exitosamente"
  
  And dentro de 10 minutos, Azure Data Factory debe validar los 100 registros (Fase 1)
  And los 100 registros deben pasar las validaciones de formato
  And debe notificar a PolicyCenter v√≠a RabbitMQ
  
  And el Workqueue Enrichment debe procesar los 100 registros (Fase 2 - Validaci√≥n)
  And todos los registros deben pasar validaciones de negocio
  And debe transformar los datos seg√∫n el template de Vida Grupo Integral
  
  And el Workqueue BulkInsert debe insertar los 100 riesgos en la p√≥liza
  And debe invocar Edge Service "Submission" para cada riesgo
  And debe crear 100 certificados individuales en PolicyCenter
  
  And al refrescar el dashboard, debe mostrar:
    | Campo | Valor Esperado |
    | Estado | Finalizada |
    | Registros Cargados | 100 |
    | Exitosos | 100 |
    | Fallidos | 0 |
    | Fase | 2 |
```

#### TC002: Manejo de Error Cr√≠tico - Validaci√≥n Fase 1

```gherkin
Scenario: Archivo con errores de formato es rechazado en Fase 1
  Given el usuario es un expedidor autenticado en PolicyCenter
  And existe una p√≥liza master "VG-2025-002"
  And el usuario tiene un archivo CSV "riesgos_invalidos.csv" con 50 registros
  And 30 registros tienen fechas de nacimiento en formato incorrecto (DD/MM/YYYY en lugar de YYYY-MM-DD)
  And 10 registros tienen identificaciones con caracteres especiales inv√°lidos
  And 10 registros son v√°lidos
  
  When el usuario carga el archivo "riesgos_invalidos.csv"
  
  Then el archivo debe cargarse exitosamente a Azure Storage
  And Azure Data Factory debe ejecutar validaciones de formato
  And debe detectar 40 errores de formato (30 fechas + 10 identificaciones)
  And debe generar archivo "ErrorFase1_{md5}.csv" con detalle de los 40 errores
  And debe almacenar el archivo en Azure Storage
  And debe actualizar estado de la carga a "ERROR_PHASE1" en Cosmos DB
  
  And al refrescar el dashboard, debe mostrar:
    | Campo | Valor Esperado |
    | Estado | Error en Fase 1 |
    | Registros Cargados | 50 |
    | Exitosos Fase 1 | 10 |
    | Fallidos Fase 1 | 40 |
  
  And el usuario debe poder hacer click en "Descargar Errores Fase 1"
  And debe descargar archivo CSV con las 40 l√≠neas con error y descripci√≥n del problema
  And debe mostrar mensajes como "Fecha inv√°lida en columna 5", "Caracteres especiales no permitidos en identificaci√≥n"
```

#### TC003: Timeout en Procesamiento

```gherkin
Scenario: Timeout en Edge Service requiere reintento autom√°tico
  Given hay una carga en Fase 2 con 20 registros en estado ACKED
  And el Workqueue BulkInsert est√° procesando los registros
  And el servicio Edge de PolicyCenter tiene alta latencia (> 30 segundos)
  
  When el Workqueue intenta insertar el registro #5
  And invoca el Edge Service "Submission"
  And el Edge Service no responde en 30 segundos (timeout)
  
  Then el Workqueue debe capturar TimeoutException
  And debe verificar que attempts < maxAttempts (3)
  And debe incrementar el contador de attempts a 1
  And debe re-encolar el registro #5 para reintentar en 5 minutos
  And debe continuar procesando el registro #6
  
  And en el segundo intento (5 minutos despu√©s)
  When el Edge Service sigue con timeout
  Then debe incrementar attempts a 2 y re-encolar
  
  And en el tercer intento
  When el Edge Service finalmente responde exitosamente
  Then debe marcar el registro como COMPLETED
  And debe actualizar estado en Azure a "Procesado"
```

---

## üîç **Troubleshooting**

### Problemas Comunes y Soluciones

#### Error: "La p√≥liza m√°ster est√° procesando otra carga y no ha finalizado"

**Causa**: Hay una carga previa en estado "En Progreso" que no ha finalizado (posiblemente atascada)

**Diagn√≥stico**:
```bash
# 1. Verificar cargas activas en Cosmos DB (Azure Portal)
# Navegar a: Cosmos DB ‚Üí MASSIVE_UPLOAD ‚Üí Container: MAIN_UPLOAD
# Query:
SELECT * FROM c WHERE c.policyNumber = "VG-2025-001" AND c.status IN ("PROCESSING_PHASE2", "VALIDATING_PHASE1")

# 2. Verificar en PolicyCenter Oracle DB
SELECT * FROM MassiveUploadMessage_Ext 
WHERE MasterPolicy = 'VG-2025-001' 
  AND Status IN ('PENDING_SEND', 'ACKED')
ORDER BY LastUpdate DESC;

# 3. Verificar estado de Workqueues
# En PolicyCenter Admin Console:
# Admin ‚Üí Operations ‚Üí WorkQueues ‚Üí Buscar "MassiveUpload"
```

**Soluci√≥n**: 
1. Si la carga realmente est√° procesando, esperar a que finalice
2. Si est√° atascada (sin cambios por > 1 hora):
   - Opci√≥n A: Actualizar estado manualmente en Cosmos DB a "COMPLETED"
   - Opci√≥n B: Usar API `PUT /massivestatus/{policyNumber}/{md5}` con `force_complete=true`
   - Opci√≥n C: Ejecutar script de limpieza en PolicyCenter

```sql
-- Script para marcar carga como completada manualmente
UPDATE MassiveUploadMessage_Ext 
SET Status = 'COMPLETED', LastUpdate = SYSDATE
WHERE LoadId = '{md5}' AND MasterPolicy = 'VG-2025-001';
COMMIT;
```

#### Error: "Archivo de errores no encontrado para Fase X"

**Causa**: El archivo de errores fue eliminado de Azure Storage o nunca se gener√≥

**Diagn√≥stico**:
```bash
# Verificar en Azure Portal
# Storage Account ‚Üí Containers ‚Üí Buscar archivo "ErrorFase1_{md5}.csv"

# Verificar linkErrors en Cosmos DB
SELECT c.linkErrors FROM c WHERE c.md5 = "{md5}"
```

**Soluci√≥n**: 
- Si el link existe pero el archivo no: Regenerar archivo de errores desde Cosmos DB
- Si no hay link: Verificar logs de Data Factory para ver si hubo errores al generar el archivo

#### Error: "Validation Exception: Riesgo no encontrado para beneficiario"

**Causa**: Carga de beneficiarios donde el riesgo asociado no existe en la p√≥liza

**Diagn√≥stico**:
```bash
# En PolicyCenter Oracle
SELECT * FROM pc_policyrisk 
WHERE PolicyNumber = 'VG-2025-001' 
  AND TaxId = '123456789';
```

**Soluci√≥n**: 
- Verificar que el archivo de beneficiarios use identificaciones de riesgos existentes
- Cargar primero los riesgos, luego los beneficiarios
- Revisar que la operaci√≥n de beneficiarios sea "Modificaci√≥n" (no "Ingreso")

### Comandos de Diagn√≥stico √ötiles

```bash
# 1. Verificar estado de componentes Azure
# Azure CLI
az functionapp show --name {env}Upload --resource-group {rg} --query "state"
az functionapp show --name {env}Status --resource-group {rg} --query "state"

# Verificar logs de Azure Functions
az functionapp log tail --name {env}Upload --resource-group {rg}

# 2. Verificar Data Factory pipeline runs
az datafactory pipeline-run query-by-factory \
  --factory-name {dataFactoryName} \
  --resource-group {rg} \
  --last-updated-after "2025-11-25T00:00:00Z"

# 3. Revisar logs de PolicyCenter Workqueues
# En servidor PolicyCenter
tail -f /path/to/PolicyCenter/logs/PolicyCenter.log | grep "MassiveUpload"

# 4. Verificar RabbitMQ queues
# RabbitMQ Management UI: http://msglab.suramericana.com.co:15672
# Verificar:
# - Queue: massive.upload.notify.phase (messages ready, unacked)
# - Queue: massive.upload.record.ready (messages ready, unacked)

# 5. Verificar m√©tricas de Cosmos DB
# Azure Portal ‚Üí Cosmos DB ‚Üí Metrics
# M√©tricas clave:
# - Total Requests
# - Throttled Requests (429 errors)
# - Server Side Latency
```

---

## üöÄ **Optimizaciones Futuras**

### Oportunidades de Mejora Identificadas

1. **Paralelizaci√≥n de Workqueues**
   - Actualmente: Procesamiento secuencial de registros en cada Workqueue
   - Mejora propuesta: Implementar procesamiento paralelo con Thread Pool
   - Beneficio esperado: Reducci√≥n de 40-60% en tiempo de Fase 2 para cargas grandes
   - Complejidad estimada: Media (requiere gesti√≥n de concurrencia y locks en DB)

2. **Cache de Validaciones**
   - Actualmente: Cada registro hace queries individuales a Cosmos/Oracle para validaciones
   - Mejora propuesta: Implementar cache distribuido (Redis) para cat√°logos, p√≥lizas master, configuraciones
   - Beneficio esperado: Reducci√≥n de 30-50% en latencia de validaciones Fase 2
   - Complejidad estimada: Media (requiere infraestructura Redis + invalidaci√≥n de cache)

3. **Streaming de Archivos (chunked upload)**
   - Actualmente: Archivo completo se sube antes de iniciar Fase 1
   - Mejora propuesta: Iniciar validaci√≥n Fase 1 mientras se est√°n subiendo bloques finales
   - Beneficio esperado: Reducci√≥n de 20-30% en tiempo total para archivos grandes (> 10MB)
   - Complejidad estimada: Alta (requiere cambios en DataFactory triggers y coordinaci√≥n)

4. **Notificaciones Push en Dashboard**
   - Actualmente: Usuario debe refrescar manualmente el dashboard
   - Mejora propuesta: Implementar WebSockets o Server-Sent Events para actualizaci√≥n en tiempo real
   - Beneficio esperado: Mejor experiencia de usuario, visibilidad instant√°nea de cambios
   - Complejidad estimada: Media (requiere infraestructura WebSocket en PolicyCenter)

5. **Auto-correcci√≥n de Errores Comunes**
   - Actualmente: Todos los errores requieren intervenci√≥n manual
   - Mejora propuesta: Implementar reglas de auto-correcci√≥n para errores comunes (ej: formato de fechas, trim de espacios)
   - Beneficio esperado: Reducci√≥n de 10-20% en tasa de error Fase 1
   - Complejidad estimada: Baja (l√≥gica de transformaci√≥n en DataFactory)

6. **Historizaci√≥n y Auditor√≠a Mejorada**
   - Actualmente: TTL de 90 d√≠as en Cosmos DB, sin hist√≥rico detallado
   - Mejora propuesta: Implementar Data Lake para almacenamiento a largo plazo de auditor√≠a
   - Beneficio esperado: Cumplimiento normativo, an√°lisis de tendencias hist√≥ricos
   - Complejidad estimada: Media (requiere pipeline ETL Cosmos ‚Üí Data Lake)

### Roadmap de Evoluci√≥n

```mermaid
timeline
    title Roadmap de Optimizaci√≥n del Flujo de Carga Masiva

    Q1 2026 : Fase de Quick Wins
            : Implementar auto-correcci√≥n de errores comunes (Fase 1)
            : Optimizar queries Cosmos DB con √≠ndices adicionales
            : Configurar alertas proactivas en Application Insights

    Q2 2026 : Fase de Performance
            : Implementar cache distribuido (Redis) para validaciones
            : Paralelizaci√≥n de Workqueues con Thread Pool
            : Optimizaci√≥n de servicios Edge (revisi√≥n de queries SQL)

    Q3 2026 : Fase de UX
            : Notificaciones push en dashboard (WebSockets)
            : Streaming de archivos con validaci√≥n temprana
            : Mejoras en UI del dashboard (filtros, b√∫squeda, exportaci√≥n)

    Q4 2026 : Fase de Governance
            : Implementar Data Lake para auditor√≠a a largo plazo
            : Dashboard anal√≠tico de tendencias y m√©tricas hist√≥ricas
            : Implementar ML para predicci√≥n de errores comunes
```

---

## üìö **Referencias**

- **GPS Arquitect√≥nico**: [Arquitectura del Sistema - GPS Principal](./index.md)
- **Documentaci√≥n de Componentes**:
  - [PolicyCenter - Arquitectura Detallada](./architecture-policycenter.md)
  - [VidaGrupoIAC - Infraestructura como C√≥digo](./architecture-VidaGrupoIAC.md)
  - [ADR-001: Refactorizaci√≥n Carga Masiva Beneficiarios Dummy](./adr-001-refactorizacion-carga-masiva-beneficiarios-dummy.md)
- **Historias de Usuario Relacionadas**:
  - [Historia #825041: Habilitar Creaci√≥n de Beneficiarios Dummy por Carga Masiva](../stories/825041.habilitar-creacion-de-beneficiarios-dummy-por-carga-masiva-en-vida-grupo.story.md)
- **Configuraci√≥n**: 
  - PolicyCenter: `./PolicyCenter/modules/configuration/config/web/pcf/`
  - VidaGrupoIAC: `./VidaGrupoIAC/modules/`
  - Azure Validations: `./VidaGrupoIAC/modules/datafactory/regex.csv`
- **C√≥digo Fuente**:
  - JavaScript Client: `./PolicyCenter/modules/configuration/deploy/resources/javascript/massiveLoad.js`
  - Workqueue Enrichment: `./PolicyCenter/modules/configuration/gsrc/sura/pc/massiveupload/batch/enrichment/EnrichmentProcessing.gs`
  - Workqueue Message Process: `./PolicyCenter/modules/configuration/gsrc/sura/pc/massiveupload/plugin/messaging/MassiveUploadMessageProcess.gs`
  - Edge Processing: `./PolicyCenter/modules/configuration/gsrc/sura/pc/massiveupload/batch/edge/EdgeProcessing.gs`
  - **Beneficiarios - Validaci√≥n**: `./PolicyCenter/modules/configuration/gsrc/sura/pc/massiveupload/validations/factories/DefaultBeneficiaryValidation.gs`
  - **Beneficiarios - Transformaci√≥n**: `./PolicyCenter/modules/configuration/gsrc/sura/pc/massiveupload/transformer/DefaultBeneficiaryTransformer.gs`
  - **Beneficiarios - Estrategias**: `./PolicyCenter/modules/configuration/gsrc/sura/pc/massiveupload/strategy/`
  - Data Factory Pipelines: `./VidaGrupoIAC/modules/datafactory/pipelines/`

---

## üìù **Historial de Cambios**

| Versi√≥n | Fecha | Autor | Descripci√≥n |
|---------|-------|-------|-------------|
| 1.0 | 25-Nov-2025 | Arquitecto | Documentaci√≥n inicial del flujo de carga masiva |
| 1.1 | 28-Ene-2026 | Developer | Agregado flujo espec√≠fico de beneficiarios con soporte para Dummy (Historia #825041) |

---

_Documentaci√≥n generada con M√©todo Ceiba - Arquitecto_  
_√öltima actualizaci√≥n: 28 de Enero, 2026_  
_Versi√≥n: 1.1_
